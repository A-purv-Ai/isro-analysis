{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5aHWAbA/t/2PaKUDbznko"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# @title BLOCK 1 - Project Information\n","\n","\"\"\"\n","ISRO Launch Market Trend Analysis & Risk Prediction\n","Student: Apurva Upadhyay (IIT Ropar, Minor in AI)\n","Notebook: 01_Data_Acquisition.ipynb\n","\n","Purpose:\n","- Scrape ISRO launch history data from Wikipedia\n","- Extract PSLV, GSLV, and LVM3 launch records\n","- Consolidate into single dataset for ML pipeline\n","\n","Run Instructions:\n","- FIRST TIME: Run all blocks sequentially (1-14)\n","- AFTER RUNTIME RESTART: Run blocks 1, 2, 14 only (skip scraping if data exists)\n","\n","Data Output:\n","- isro_launch_history_raw.csv (91 missions)\n","\"\"\"\n","\n","print(\"ISRO Launch Data Acquisition Pipeline\")\n","print(\"Student: Apurva Upadhyay\")\n"],"metadata":{"id":"iO8W3NH7SN0V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769465117534,"user_tz":-330,"elapsed":6,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"33f0d518-c074-422d-c253-34b6e8c91883"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["ISRO Launch Data Acquisition Pipeline\n","Student: Apurva Upadhyay\n"]}]},{"cell_type":"code","source":["# @title BLOCK 2 - Mount Google Drive (RUN EVERY SESSION)\n","\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","# Define project paths\n","BASE_PATH = '/content/drive/My Drive/Course/Minor in AI/Final Project/ISRO Launch Trend Analysis - Apurva Upadhyay'\n","DATASET_PATH = os.path.join(BASE_PATH, 'Dataset')\n","CODE_PATH = os.path.join(BASE_PATH, 'Code')\n","OUTPUT_PATH = os.path.join(BASE_PATH, 'Output')\n","\n","# Create directories\n","os.makedirs(DATASET_PATH, exist_ok=True)\n","os.makedirs(os.path.join(DATASET_PATH, 'individual'), exist_ok=True)\n","os.makedirs(CODE_PATH, exist_ok=True)\n","os.makedirs(OUTPUT_PATH, exist_ok=True)\n","os.makedirs('isro_scraper', exist_ok=True)\n","\n","print(\"Google Drive mounted successfully\")\n","print(f\"Dataset path: {DATASET_PATH}\")\n","print(f\"Code path: {CODE_PATH}\")\n","print(\"Directory structure created\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mI6cbHJVvJI","executionInfo":{"status":"ok","timestamp":1769465161572,"user_tz":-330,"elapsed":20589,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"533e9ab1-3e78-4d28-82a7-58f0188f17ad"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive mounted successfully\n","Dataset path: /content/drive/My Drive/Course/Minor in AI/Final Project/ISRO Launch Trend Analysis - Apurva Upadhyay/Dataset\n","Code path: /content/drive/My Drive/Course/Minor in AI/Final Project/ISRO Launch Trend Analysis - Apurva Upadhyay/Code\n","Directory structure created\n"]}]},{"cell_type":"code","source":["# @title BLOCK 3 - Install Dependencies (RUN ONCE)\n","\n","!pip install requests beautifulsoup4 pydantic pandas -q\n","\n","print(\"Dependencies installed\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbjDGghgWDXb","executionInfo":{"status":"ok","timestamp":1769465219402,"user_tz":-330,"elapsed":5495,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"480a6023-9504-4355-992b-ef6a623f5654"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Dependencies installed\n"]}]},{"cell_type":"code","source":["# @title BLOCK 4 - Create config.py\n","\n","%%writefile isro_scraper/config.py\n","\"\"\"Configuration module for ISRO Launch Scraper.\"\"\"\n","\n","from typing import Dict\n","from dataclasses import dataclass\n","\n","\n","@dataclass\n","class ScraperConfig:\n","    \"\"\"Main configuration for the scraper application.\"\"\"\n","\n","    REQUEST_TIMEOUT: int = 30\n","    MAX_RETRIES: int = 3\n","    RETRY_DELAY: int = 5\n","    INTER_SCRAPER_DELAY: int = 10\n","\n","    HEADERS: Dict[str, str] = None\n","\n","    def __post_init__(self):\n","        \"\"\"Initialize headers after dataclass creation.\"\"\"\n","        if self.HEADERS is None:\n","            self.HEADERS = {\n","                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n","                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n","                'Accept-Language': 'en-US,en;q=0.9',\n","                'Accept-Encoding': 'gzip, deflate, br',\n","                'DNT': '1',\n","                'Connection': 'keep-alive',\n","                'Upgrade-Insecure-Requests': '1',\n","                'Sec-Fetch-Dest': 'document',\n","                'Sec-Fetch-Mode': 'navigate',\n","                'Sec-Fetch-Site': 'none',\n","                'Cache-Control': 'max-age=0'\n","            }\n","\n","\n","WIKIPEDIA_URLS = {\n","    'pslv': 'https://en.wikipedia.org/wiki/List_of_PSLV_launches',\n","    'gslv': 'https://en.wikipedia.org/wiki/List_of_GSLV_launches',\n","    'lvm3': 'https://en.wikipedia.org/wiki/List_of_LVM3_launches'\n","}\n","\n","EXPECTED_COUNTS = {\n","    'pslv': 64,\n","    'gslv': 18,\n","    'lvm3': 9\n","}\n","\n","VALID_OUTCOMES = {\n","    'Success', 'Failure', 'Partial failure', 'Partial Failure',\n","    'Scheduled', 'Cancelled', 'Planned'\n","}\n","\n","OUTPUT_DIR = 'output'\n","LOG_DIR = 'logs'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ttQ-TldWR6H","executionInfo":{"status":"ok","timestamp":1769465262778,"user_tz":-330,"elapsed":13,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"870d54c4-c7f3-42b5-f399-377f28e2d13f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing isro_scraper/config.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 5 - Create models.py\n","\n","%%writefile isro_scraper/models.py\n","\"\"\"Data models for ISRO launch records.\"\"\"\n","\n","from typing import Optional\n","from pydantic import BaseModel, Field, field_validator\n","import re\n","\n","\n","class PSLVLaunch(BaseModel):\n","    \"\"\"Data model for PSLV launch records.\"\"\"\n","\n","    flight_number: str = Field(..., description=\"Flight number\")\n","    date_time_utc: Optional[str] = Field(None, description=\"Launch date/time UTC\")\n","    rocket_configuration: Optional[str] = Field(None, description=\"Rocket variant\")\n","    launch_site: Optional[str] = Field(None, description=\"Launch site\")\n","    payload: str = Field(..., description=\"Payload name(s)\")\n","    payload_mass: Optional[str] = Field(None, description=\"Payload mass\")\n","    orbit: Optional[str] = Field(None, description=\"Target orbit\")\n","    user: Optional[str] = Field(None, description=\"Customer/Organization\")\n","    launch_outcome: Optional[str] = Field(None, description=\"Outcome\")\n","    remarks: Optional[str] = Field(None, description=\"Mission notes\")\n","\n","    @field_validator('payload_mass')\n","    @classmethod\n","    def validate_mass(cls, v: Optional[str]) -> Optional[str]:\n","        \"\"\"Validate mass contains numeric values.\"\"\"\n","        if v is None or v.strip() == '':\n","            return None\n","        if re.search(r'\\d', v):\n","            return v.strip()\n","        return None\n","\n","\n","class GSLVLaunch(BaseModel):\n","    \"\"\"Data model for GSLV launch records.\"\"\"\n","\n","    flight_number: str = Field(..., description=\"Flight number\")\n","    date_time_utc: Optional[str] = Field(None, description=\"Launch date/time UTC\")\n","    rocket_configuration: Optional[str] = Field(None, description=\"Rocket variant\")\n","    launch_site: Optional[str] = Field(None, description=\"Launch site\")\n","    payload: str = Field(..., description=\"Payload name(s)\")\n","    payload_mass: Optional[str] = Field(None, description=\"Payload mass\")\n","    orbit: Optional[str] = Field(None, description=\"Target orbit\")\n","    user: Optional[str] = Field(None, description=\"Customer/Organization\")\n","    launch_outcome: Optional[str] = Field(None, description=\"Outcome\")\n","    remarks: Optional[str] = Field(None, description=\"Mission notes\")\n","\n","    @field_validator('payload_mass')\n","    @classmethod\n","    def validate_mass(cls, v: Optional[str]) -> Optional[str]:\n","        \"\"\"Validate mass contains numeric values.\"\"\"\n","        if v is None or v.strip() == '':\n","            return None\n","        if re.search(r'\\d', v):\n","            return v.strip()\n","        return None\n","\n","\n","class LVM3Launch(BaseModel):\n","    \"\"\"Data model for LVM3 launch records.\"\"\"\n","\n","    flight_number: str = Field(..., description=\"Flight number\")\n","    date_time_utc: Optional[str] = Field(None, description=\"Launch date/time UTC\")\n","    launch_site: Optional[str] = Field(None, description=\"Launch site\")\n","    payload: str = Field(..., description=\"Payload name(s)\")\n","    payload_mass: Optional[str] = Field(None, description=\"Payload mass with units\")\n","    regime: Optional[str] = Field(None, description=\"Orbital regime\")\n","    operator: Optional[str] = Field(None, description=\"Operator/Organization\")\n","    function: Optional[str] = Field(None, description=\"Mission function\")\n","    status: Optional[str] = Field(None, description=\"Launch status\")\n","    remarks: Optional[str] = Field(None, description=\"Mission notes\")\n","\n","    @field_validator('payload_mass')\n","    @classmethod\n","    def validate_mass(cls, v: Optional[str]) -> Optional[str]:\n","        \"\"\"Validate mass contains numeric values.\"\"\"\n","        if v is None or v.strip() == '':\n","            return None\n","        if re.search(r'\\d', v):\n","            return v.strip()\n","        return None\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5bEKG6jEWd9S","executionInfo":{"status":"ok","timestamp":1769465310848,"user_tz":-330,"elapsed":26,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"69574b80-0964-49ac-b5f8-b9f90d751892"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing isro_scraper/models.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 6 - Create parsers.py\n","\n","%%writefile isro_scraper/parsers.py\n","\"\"\"HTML parsing utilities.\"\"\"\n","\n","from typing import Optional\n","from bs4 import Tag\n","import re\n","\n","\n","def clean_text(text: Optional[str]) -> Optional[str]:\n","    \"\"\"Clean and normalize text from HTML.\"\"\"\n","    if text is None:\n","        return None\n","    text = re.sub(r'\\[\\d+\\]', '', text)\n","    text = ' '.join(text.split())\n","    text = text.strip()\n","    return text if text else None\n","\n","\n","def extract_cell_text(cell: Tag) -> Optional[str]:\n","    \"\"\"Extract text from table cell.\"\"\"\n","    if cell is None:\n","        return None\n","    text = cell.get_text(separator='\\n', strip=True)\n","    return clean_text(text)\n","\n","\n","def is_remark_row(row: Tag) -> bool:\n","    \"\"\"Check if row is a remarks row.\"\"\"\n","    cells = row.find_all(['td', 'th'])\n","    if len(cells) == 1 and cells[0].has_attr('colspan'):\n","        return True\n","    return False\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UK60noR5Wi-x","executionInfo":{"status":"ok","timestamp":1769465331197,"user_tz":-330,"elapsed":70,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"5f16efa5-72e7-45ef-e9af-1c03d0f71001"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing isro_scraper/parsers.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 7 - Create exporters.py\n","\n","%%writefile isro_scraper/exporters.py\n","\"\"\"Export utilities for launch data.\"\"\"\n","\n","import csv\n","import json\n","from typing import List, Union\n","from pathlib import Path\n","import logging\n","\n","from .models import PSLVLaunch, GSLVLaunch, LVM3Launch\n","\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","\n","def export_to_csv(launches: List[Union[PSLVLaunch, GSLVLaunch, LVM3Launch]], filepath: str) -> None:\n","    \"\"\"Export launch data to CSV file.\"\"\"\n","    if not launches:\n","        logger.warning(\"No data to export\")\n","        return\n","\n","    Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n","\n","    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n","        fieldnames = list(launches[0].model_dump().keys())\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","\n","        for launch in launches:\n","            writer.writerow(launch.model_dump())\n","\n","    logger.info(f\"Exported {len(launches)} records to {filepath}\")\n","\n","\n","def export_to_json(launches: List[Union[PSLVLaunch, GSLVLaunch, LVM3Launch]], filepath: str) -> None:\n","    \"\"\"Export launch data to JSON file.\"\"\"\n","    if not launches:\n","        logger.warning(\"No data to export\")\n","        return\n","\n","    Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n","\n","    outcomes = {}\n","    for launch in launches:\n","        if hasattr(launch, 'launch_outcome'):\n","            outcome = launch.launch_outcome or 'Unknown'\n","        elif hasattr(launch, 'status'):\n","            outcome = launch.status or 'Unknown'\n","        else:\n","            outcome = 'Unknown'\n","\n","        outcomes[outcome] = outcomes.get(outcome, 0) + 1\n","\n","    data = {\n","        'metadata': {\n","            'total_launches': len(launches),\n","            'outcomes': outcomes\n","        },\n","        'launches': [launch.model_dump() for launch in launches]\n","    }\n","\n","    with open(filepath, 'w', encoding='utf-8') as jsonfile:\n","        json.dump(data, jsonfile, indent=2, ensure_ascii=False)\n","\n","    logger.info(f\"Exported {len(launches)} records to {filepath}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HAV8-CZWmBG","executionInfo":{"status":"ok","timestamp":1769465342762,"user_tz":-330,"elapsed":6,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"fb319e01-a4d0-4eea-b0b0-faa8fc5934a2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing isro_scraper/exporters.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 8 - Create pslv_scraper.py\n","\n","%%writefile isro_scraper/pslv_scraper.py\n","\"\"\"PSLV Launch Scraper Module.\"\"\"\n","\n","import requests\n","from bs4 import BeautifulSoup, Tag\n","from typing import List, Optional, Dict\n","import logging\n","from time import sleep\n","\n","from .config import ScraperConfig, WIKIPEDIA_URLS\n","from .models import PSLVLaunch\n","from .parsers import clean_text, extract_cell_text, is_remark_row\n","\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","\n","class PSLVScraper:\n","    \"\"\"Scraper for PSLV launch data from Wikipedia.\"\"\"\n","\n","    def __init__(self, config: Optional[ScraperConfig] = None):\n","        \"\"\"Initialize PSLV scraper with configuration.\"\"\"\n","        self.config = config or ScraperConfig()\n","        self.launches: List[PSLVLaunch] = []\n","        self.url = WIKIPEDIA_URLS['pslv']\n","\n","    def scrape(self) -> List[PSLVLaunch]:\n","        \"\"\"Scrape PSLV launch data from Wikipedia.\"\"\"\n","        self.launches = []\n","\n","        for attempt in range(1, self.config.MAX_RETRIES + 1):\n","            try:\n","                logger.info(f\"Fetching PSLV data (attempt {attempt}/{self.config.MAX_RETRIES})\")\n","                response = requests.get(\n","                    self.url,\n","                    headers=self.config.HEADERS,\n","                    timeout=self.config.REQUEST_TIMEOUT\n","                )\n","                response.raise_for_status()\n","                logger.info(f\"Successfully fetched page ({len(response.text):,} bytes)\")\n","                break\n","            except requests.exceptions.RequestException as e:\n","                if attempt == self.config.MAX_RETRIES:\n","                    logger.error(f\"Failed to fetch after {self.config.MAX_RETRIES} attempts: {e}\")\n","                    raise\n","                logger.warning(f\"Attempt {attempt} failed, retrying in {self.config.RETRY_DELAY}s...\")\n","                sleep(self.config.RETRY_DELAY)\n","\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        tables = soup.find_all('table', class_='wikitable')\n","        logger.info(f\"Found {len(tables)} launch tables\")\n","\n","        for table_idx, table in enumerate(tables):\n","            self._parse_table(table, table_idx)\n","\n","        logger.info(f\"Scraped {len(self.launches)} PSLV launches\")\n","        return self.launches\n","\n","    def _parse_table(self, table: Tag, table_idx: int) -> None:\n","        \"\"\"Parse a single PSLV launch table.\"\"\"\n","        rows = table.find_all('tr')\n","\n","        for row_idx, row in enumerate(rows[2:], start=2):\n","            if is_remark_row(row):\n","                if self.launches:\n","                    remark_text = extract_cell_text(row.find('td'))\n","                    if remark_text:\n","                        self.launches[-1].remarks = remark_text\n","                continue\n","\n","            cells = row.find_all('td')\n","            if len(cells) < 8:\n","                continue\n","\n","            flight_th = row.find('th')\n","            flight_number = extract_cell_text(flight_th) if flight_th else None\n","\n","            if not flight_number:\n","                continue\n","\n","            try:\n","                launch = PSLVLaunch(\n","                    flight_number=flight_number,\n","                    date_time_utc=extract_cell_text(cells[0]),\n","                    rocket_configuration=extract_cell_text(cells[1]),\n","                    launch_site=extract_cell_text(cells[2]),\n","                    payload=extract_cell_text(cells[3]),\n","                    payload_mass=extract_cell_text(cells[4]),\n","                    orbit=extract_cell_text(cells[5]) or None,\n","                    user=extract_cell_text(cells[6]) or None,\n","                    launch_outcome=extract_cell_text(cells[7]) if len(cells) > 7 else None,\n","                    remarks=None\n","                )\n","                self.launches.append(launch)\n","            except (IndexError, ValueError) as e:\n","                logger.debug(f\"Error parsing row {row_idx}: {e}\")\n","                continue\n","\n","    def get_statistics(self) -> Dict[str, int]:\n","        \"\"\"Calculate launch outcome statistics.\"\"\"\n","        stats = {\n","            'total': len(self.launches),\n","            'success': 0,\n","            'failure': 0,\n","            'partial_failure': 0,\n","            'scheduled': 0,\n","            'unknown': 0\n","        }\n","\n","        for launch in self.launches:\n","            outcome = launch.launch_outcome\n","            if not outcome:\n","                stats['unknown'] += 1\n","            elif outcome == 'Success':\n","                stats['success'] += 1\n","            elif outcome == 'Failure':\n","                stats['failure'] += 1\n","            elif 'Partial' in outcome:\n","                stats['partial_failure'] += 1\n","            elif outcome == 'Scheduled':\n","                stats['scheduled'] += 1\n","            else:\n","                stats['unknown'] += 1\n","\n","        return stats\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycVG7ogEXBgT","executionInfo":{"status":"ok","timestamp":1769465458673,"user_tz":-330,"elapsed":22,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"78f12c75-32d2-4ffe-8178-2890d9d56230"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing isro_scraper/pslv_scraper.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 9 - Create gslv_scraper.py\n","\n","%%writefile isro_scraper/gslv_scraper.py\n","\"\"\"GSLV Launch Scraper Module.\"\"\"\n","\n","import requests\n","from bs4 import BeautifulSoup, Tag\n","from typing import List, Optional, Dict\n","import logging\n","from time import sleep\n","\n","from .config import ScraperConfig, WIKIPEDIA_URLS\n","from .models import GSLVLaunch\n","from .parsers import clean_text, extract_cell_text, is_remark_row\n","\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","\n","class GSLVScraper:\n","    \"\"\"Scraper for GSLV launch data from Wikipedia.\"\"\"\n","\n","    def __init__(self, config: Optional[ScraperConfig] = None):\n","        \"\"\"Initialize GSLV scraper with configuration.\"\"\"\n","        self.config = config or ScraperConfig()\n","        self.launches: List[GSLVLaunch] = []\n","        self.url = WIKIPEDIA_URLS['gslv']\n","\n","    def scrape(self) -> List[GSLVLaunch]:\n","        \"\"\"Scrape GSLV launch data from Wikipedia.\"\"\"\n","        self.launches = []\n","\n","        for attempt in range(1, self.config.MAX_RETRIES + 1):\n","            try:\n","                logger.info(f\"Fetching GSLV data (attempt {attempt}/{self.config.MAX_RETRIES})\")\n","                response = requests.get(\n","                    self.url,\n","                    headers=self.config.HEADERS,\n","                    timeout=self.config.REQUEST_TIMEOUT\n","                )\n","                response.raise_for_status()\n","                logger.info(f\"Successfully fetched page ({len(response.text):,} bytes)\")\n","                break\n","            except requests.exceptions.RequestException as e:\n","                if attempt == self.config.MAX_RETRIES:\n","                    logger.error(f\"Failed to fetch after {self.config.MAX_RETRIES} attempts: {e}\")\n","                    raise\n","                logger.warning(f\"Attempt {attempt} failed, retrying in {self.config.RETRY_DELAY}s...\")\n","                sleep(self.config.RETRY_DELAY)\n","\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        tables = soup.find_all('table', class_='wikitable')\n","        logger.info(f\"Found {len(tables)} launch tables\")\n","\n","        for table_idx, table in enumerate(tables):\n","            self._parse_table(table, table_idx)\n","\n","        logger.info(f\"Scraped {len(self.launches)} GSLV launches\")\n","        return self.launches\n","\n","    def _parse_table(self, table: Tag, table_idx: int) -> None:\n","        \"\"\"Parse a single GSLV launch table.\"\"\"\n","        rows = table.find_all('tr')\n","\n","        for row_idx, row in enumerate(rows[2:], start=2):\n","            if is_remark_row(row):\n","                if self.launches:\n","                    remark_text = extract_cell_text(row.find('td'))\n","                    if remark_text:\n","                        self.launches[-1].remarks = remark_text\n","                continue\n","\n","            cells = row.find_all('td')\n","            if len(cells) < 8:\n","                continue\n","\n","            flight_th = row.find('th')\n","            flight_number = extract_cell_text(flight_th) if flight_th else None\n","\n","            if not flight_number:\n","                continue\n","\n","            try:\n","                launch = GSLVLaunch(\n","                    flight_number=flight_number,\n","                    date_time_utc=extract_cell_text(cells[0]),\n","                    rocket_configuration=extract_cell_text(cells[1]),\n","                    launch_site=extract_cell_text(cells[2]),\n","                    payload=extract_cell_text(cells[3]),\n","                    payload_mass=extract_cell_text(cells[4]),\n","                    orbit=extract_cell_text(cells[5]) or None,\n","                    user=extract_cell_text(cells[6]) or None,\n","                    launch_outcome=extract_cell_text(cells[7]) if len(cells) > 7 else None,\n","                    remarks=None\n","                )\n","                self.launches.append(launch)\n","            except (IndexError, ValueError) as e:\n","                logger.debug(f\"Error parsing row {row_idx}: {e}\")\n","                continue\n","\n","    def get_statistics(self) -> Dict[str, int]:\n","        \"\"\"Calculate launch outcome statistics.\"\"\"\n","        stats = {\n","            'total': len(self.launches),\n","            'success': 0,\n","            'failure': 0,\n","            'partial_failure': 0,\n","            'scheduled': 0,\n","            'unknown': 0\n","        }\n","\n","        for launch in self.launches:\n","            outcome = launch.launch_outcome\n","            if not outcome:\n","                stats['unknown'] += 1\n","            elif outcome == 'Success':\n","                stats['success'] += 1\n","            elif outcome == 'Failure':\n","                stats['failure'] += 1\n","            elif 'Partial' in outcome:\n","                stats['partial_failure'] += 1\n","            elif outcome == 'Scheduled':\n","                stats['scheduled'] += 1\n","            else:\n","                stats['unknown'] += 1\n","\n","        return stats\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6Zn3c-CXFJK","executionInfo":{"status":"ok","timestamp":1769465485746,"user_tz":-330,"elapsed":19,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"9185a691-443f-4aa3-f764-65eb700d225d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing isro_scraper/gslv_scraper.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 10 - Create lvm3_scraper.py\n","\n","%%writefile isro_scraper/lvm3_scraper.py\n","\"\"\"LVM3 Launch Scraper Module.\"\"\"\n","\n","import requests\n","from bs4 import BeautifulSoup, Tag\n","from typing import List, Optional, Dict\n","import logging\n","from time import sleep\n","\n","from .config import ScraperConfig, WIKIPEDIA_URLS\n","from .models import LVM3Launch\n","from .parsers import clean_text, extract_cell_text\n","\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","\n","class LVM3Scraper:\n","    \"\"\"Scraper for LVM3 launch data from Wikipedia.\"\"\"\n","\n","    def __init__(self, config: Optional[ScraperConfig] = None):\n","        \"\"\"Initialize LVM3 scraper with configuration.\"\"\"\n","        self.config = config or ScraperConfig()\n","        self.launches: List[LVM3Launch] = []\n","        self.url = WIKIPEDIA_URLS['lvm3']\n","\n","    def scrape(self) -> List[LVM3Launch]:\n","        \"\"\"Scrape LVM3 launch data from Wikipedia.\"\"\"\n","        self.launches = []\n","\n","        for attempt in range(1, self.config.MAX_RETRIES + 1):\n","            try:\n","                logger.info(f\"Fetching LVM3 data (attempt {attempt}/{self.config.MAX_RETRIES})\")\n","                response = requests.get(\n","                    self.url,\n","                    headers=self.config.HEADERS,\n","                    timeout=self.config.REQUEST_TIMEOUT\n","                )\n","                response.raise_for_status()\n","                logger.info(f\"Successfully fetched page ({len(response.text):,} bytes)\")\n","                break\n","            except requests.exceptions.RequestException as e:\n","                if attempt == self.config.MAX_RETRIES:\n","                    logger.error(f\"Failed to fetch after {self.config.MAX_RETRIES} attempts: {e}\")\n","                    raise\n","                logger.warning(f\"Attempt {attempt} failed, retrying in {self.config.RETRY_DELAY}s...\")\n","                sleep(self.config.RETRY_DELAY)\n","\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        tables = soup.find_all('table', class_='wikitable')\n","        logger.info(f\"Found {len(tables)} tables\")\n","\n","        for table_idx in [1, 2]:\n","            if table_idx < len(tables):\n","                self._parse_hierarchical_table(tables[table_idx], table_idx)\n","\n","        logger.info(f\"Scraped {len(self.launches)} LVM3 launches\")\n","        return self.launches\n","\n","    def _parse_hierarchical_table(self, table: Tag, table_idx: int) -> None:\n","        \"\"\"Parse LVM3 hierarchical multi-row table structure.\"\"\"\n","        rows = table.find_all('tr')\n","        current_launch = {}\n","\n","        for row_idx, row in enumerate(rows[4:], start=4):\n","            cells = row.find_all('td')\n","\n","            if len(cells) == 0:\n","                continue\n","\n","            if len(cells) == 1 and cells[0].get('colspan'):\n","                if current_launch and current_launch.get('flight_number'):\n","                    status = current_launch.get('status', '')\n","                    if status == 'Success':\n","                        current_launch['remarks'] = extract_cell_text(cells[0])\n","                        try:\n","                            launch = LVM3Launch(**current_launch)\n","                            self.launches.append(launch)\n","                        except Exception as e:\n","                            logger.debug(f\"Error creating launch: {e}\")\n","                current_launch = {}\n","                continue\n","\n","            if len(cells) == 5:\n","                if current_launch and current_launch.get('flight_number'):\n","                    status = current_launch.get('status', '')\n","                    if status == 'Success':\n","                        try:\n","                            launch = LVM3Launch(**current_launch)\n","                            self.launches.append(launch)\n","                        except Exception as e:\n","                            logger.debug(f\"Error creating launch: {e}\")\n","                current_launch = {}\n","\n","                date_text = extract_cell_text(cells[0])\n","                payload_raw = extract_cell_text(cells[1])\n","\n","                payload_name = payload_raw\n","                payload_mass = None\n","\n","                if payload_raw and '\\n' in payload_raw:\n","                    parts = payload_raw.split('\\n')\n","                    payload_name = parts[0].strip()\n","                    if len(parts) > 1:\n","                        payload_mass = parts[1].strip()\n","\n","                current_launch = {\n","                    'date_time_utc': date_text,\n","                    'payload': payload_name,\n","                    'payload_mass': payload_mass,\n","                    'launch_site': extract_cell_text(cells[2]),\n","                    'regime': extract_cell_text(cells[3]),\n","                    'status': extract_cell_text(cells[4]),\n","                    'flight_number': None,\n","                    'operator': None,\n","                    'function': None,\n","                    'remarks': None\n","                }\n","\n","            elif len(cells) == 3 and current_launch:\n","                current_launch['flight_number'] = extract_cell_text(cells[0])\n","                current_launch['operator'] = extract_cell_text(cells[1])\n","                current_launch['function'] = extract_cell_text(cells[2])\n","\n","        if current_launch and current_launch.get('flight_number'):\n","            status = current_launch.get('status', '')\n","            if status == 'Success':\n","                try:\n","                    launch = LVM3Launch(**current_launch)\n","                    self.launches.append(launch)\n","                except Exception as e:\n","                    logger.debug(f\"Error creating final launch: {e}\")\n","\n","    def get_statistics(self) -> Dict[str, int]:\n","        \"\"\"Calculate launch status statistics.\"\"\"\n","        stats = {\n","            'total': len(self.launches),\n","            'success': 0,\n","            'failure': 0,\n","            'partial_failure': 0,\n","            'scheduled': 0,\n","            'unknown': 0\n","        }\n","\n","        for launch in self.launches:\n","            status = launch.status\n","            if not status:\n","                stats['unknown'] += 1\n","            elif status == 'Success':\n","                stats['success'] += 1\n","            elif status == 'Failure':\n","                stats['failure'] += 1\n","            elif 'Partial' in status:\n","                stats['partial_failure'] += 1\n","            elif status in ['Scheduled', 'Planned']:\n","                stats['scheduled'] += 1\n","            else:\n","                stats['unknown'] += 1\n","\n","        return stats\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W00fbDAVXKls","executionInfo":{"status":"ok","timestamp":1769465732299,"user_tz":-330,"elapsed":13,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"09d7fffb-bf1d-4ad2-9e9c-b4d3fdc9f567"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting isro_scraper/lvm3_scraper.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 11 - Create __init__.py\n","\n","%%writefile isro_scraper/__init__.py\n","\"\"\"ISRO Launch Scraper Package.\"\"\"\n","\n","from .pslv_scraper import PSLVScraper\n","from .gslv_scraper import GSLVScraper\n","from .lvm3_scraper import LVM3Scraper\n","from .models import PSLVLaunch, GSLVLaunch, LVM3Launch\n","from .config import ScraperConfig\n","\n","__all__ = [\n","    'PSLVScraper', 'GSLVScraper', 'LVM3Scraper',\n","    'PSLVLaunch', 'GSLVLaunch', 'LVM3Launch',\n","    'ScraperConfig'\n","]\n","__version__ = '1.0.0'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8q_3c5aYXOU","executionInfo":{"status":"ok","timestamp":1769465806774,"user_tz":-330,"elapsed":45,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"7e7e88e2-62de-4502-ac29-9dc0390876b9"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing isro_scraper/__init__.py\n"]}]},{"cell_type":"code","source":["# @title BLOCK 12 - Scrape PSLV, GSLV, LVM3 (RUN ONCE OR IF DATA MISSING)\n","\n","# Note: Check if CSVs exist before scraping to avoid Wikipedia rate limits\n","# Add delays between scrapers to respect robot policy\n","\n","import sys\n","import os\n","from time import sleep\n","sys.path.insert(0, '/content')\n","\n","# Force module reload\n","for module in list(sys.modules.keys()):\n","    if 'isro_scraper' in module:\n","        del sys.modules[module]\n","\n","from isro_scraper import PSLVScraper, GSLVScraper, LVM3Scraper\n","from isro_scraper.exporters import export_to_csv, export_to_json\n","import pandas as pd\n","\n","print(\"=\" * 80)\n","print(\"ISRO LAUNCH DATA SCRAPING\")\n","print(\"=\" * 80)\n","\n","# Create output directory\n","os.makedirs('output', exist_ok=True)\n","\n","# Check-then-scrape logic\n","PSLV_CSV = 'output/pslv_launches.csv'\n","GSLV_CSV = 'output/gslv_launches.csv'\n","LVM3_CSV = 'output/lvm3_launches.csv'\n","\n","results = {}\n","\n","# Scrape PSLV\n","if os.path.exists(PSLV_CSV):\n","    print(\"\\nPSLV data already exists - skipping scrape\")\n","    pslv_df = pd.read_csv(PSLV_CSV)\n","    results['pslv'] = len(pslv_df)\n","else:\n","    print(\"\\nScraping PSLV launches...\")\n","    scraper = PSLVScraper()\n","    launches = scraper.scrape()\n","    export_to_csv(launches, PSLV_CSV)\n","    export_to_json(launches, 'output/pslv_launches.json')\n","    results['pslv'] = len(launches)\n","    print(f\"PSLV: {len(launches)} launches scraped\")\n","    sleep(10)\n","\n","# Scrape GSLV\n","if os.path.exists(GSLV_CSV):\n","    print(\"\\nGSLV data already exists - skipping scrape\")\n","    gslv_df = pd.read_csv(GSLV_CSV)\n","    results['gslv'] = len(gslv_df)\n","else:\n","    print(\"\\nScraping GSLV launches...\")\n","    scraper = GSLVScraper()\n","    launches = scraper.scrape()\n","    export_to_csv(launches, GSLV_CSV)\n","    export_to_json(launches, 'output/gslv_launches.json')\n","    results['gslv'] = len(launches)\n","    print(f\"GSLV: {len(launches)} launches scraped\")\n","    sleep(10)\n","\n","# Scrape LVM3\n","if os.path.exists(LVM3_CSV):\n","    print(\"\\nLVM3 data already exists - skipping scrape\")\n","    lvm3_df = pd.read_csv(LVM3_CSV)\n","    results['lvm3'] = len(lvm3_df)\n","else:\n","    print(\"\\nScraping LVM3 launches...\")\n","    scraper = LVM3Scraper()\n","    launches = scraper.scrape()\n","    export_to_csv(launches, LVM3_CSV)\n","    export_to_json(launches, 'output/lvm3_launches.json')\n","    results['lvm3'] = len(launches)\n","    print(f\"LVM3: {len(launches)} launches scraped\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"SCRAPING COMPLETE\")\n","print(\"=\" * 80)\n","print(f\"PSLV: {results['pslv']} launches\")\n","print(f\"GSLV: {results['gslv']} launches\")\n","print(f\"LVM3: {results['lvm3']} launches\")\n","print(f\"Total: {sum(results.values())} launches\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yghs0FamYa4w","executionInfo":{"status":"ok","timestamp":1769465846224,"user_tz":-330,"elapsed":25221,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"d87f8598-ba1e-4105-9b16-ec378f72bf52"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","ISRO LAUNCH DATA SCRAPING\n","================================================================================\n","\n","Scraping PSLV launches...\n","PSLV: 64 launches scraped\n","\n","Scraping GSLV launches...\n","GSLV: 18 launches scraped\n","\n","Scraping LVM3 launches...\n","LVM3: 9 launches scraped\n","\n","================================================================================\n","SCRAPING COMPLETE\n","================================================================================\n","PSLV: 64 launches\n","GSLV: 18 launches\n","LVM3: 9 launches\n","Total: 91 launches\n"]}]},{"cell_type":"code","source":["# @title BLOCK 13 - Consolidate All Rockets (RUN EVERY SESSION)\n","\n","# Note: Merge PSLV, GSLV, LVM3 with standardized columns\n","\n","import pandas as pd\n","\n","print(\"=\" * 80)\n","print(\"DATA CONSOLIDATION\")\n","print(\"=\" * 80)\n","\n","# Load individual datasets\n","pslv_df = pd.read_csv('output/pslv_launches.csv')\n","gslv_df = pd.read_csv('output/gslv_launches.csv')\n","lvm3_df = pd.read_csv('output/lvm3_launches.csv')\n","\n","print(f\"\\nLoaded datasets:\")\n","print(f\"  PSLV - {len(pslv_df)} launches\")\n","print(f\"  GSLV - {len(gslv_df)} launches\")\n","print(f\"  LVM3 - {len(lvm3_df)} launches\")\n","\n","# Add rocket_type column\n","pslv_df.insert(0, 'rocket_type', 'PSLV')\n","gslv_df.insert(0, 'rocket_type', 'GSLV')\n","lvm3_df.insert(0, 'rocket_type', 'LVM3')\n","\n","# Standardize LVM3 columns to match PSLV/GSLV\n","lvm3_df = lvm3_df.rename(columns={\n","    'status': 'launch_outcome',\n","    'regime': 'orbit',\n","    'operator': 'user'\n","})\n","\n","# Add missing columns\n","lvm3_df['rocket_configuration'] = None\n","pslv_df['function'] = None\n","gslv_df['function'] = None\n","\n","# Standard column order\n","standard_columns = [\n","    'rocket_type',\n","    'flight_number',\n","    'date_time_utc',\n","    'rocket_configuration',\n","    'launch_site',\n","    'payload',\n","    'payload_mass',\n","    'orbit',\n","    'user',\n","    'launch_outcome',\n","    'function',\n","    'remarks'\n","]\n","\n","# Reorder columns\n","pslv_df = pslv_df[standard_columns]\n","gslv_df = gslv_df[standard_columns]\n","lvm3_df = lvm3_df[standard_columns]\n","\n","# Concatenate all three\n","combined_df = pd.concat([pslv_df, gslv_df, lvm3_df], ignore_index=True)\n","\n","# Sort by date\n","combined_df['date_parsed'] = pd.to_datetime(combined_df['date_time_utc'], errors='coerce')\n","combined_df = combined_df.sort_values('date_parsed')\n","combined_df = combined_df.drop('date_parsed', axis=1)\n","combined_df = combined_df.reset_index(drop=True)\n","\n","# Save consolidated dataset\n","combined_df.to_csv('output/isro_launch_history_raw.csv', index=False)\n","\n","print(f\"\\nConsolidated dataset:\")\n","print(f\"  Total missions - {len(combined_df)}\")\n","print(f\"  Date range - {combined_df['date_time_utc'].iloc[0]} to {combined_df['date_time_utc'].iloc[-1]}\")\n","\n","print(f\"\\nOutcome distribution:\")\n","print(combined_df['launch_outcome'].value_counts())\n","\n","print(\"\\nFile saved - output/isro_launch_history_raw.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bb69M6zBYxIb","executionInfo":{"status":"ok","timestamp":1769465926871,"user_tz":-330,"elapsed":65,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"20a51d30-284e-4bc9-ca7d-2ff888a50699"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","DATA CONSOLIDATION\n","================================================================================\n","\n","Loaded datasets:\n","  PSLV - 64 launches\n","  GSLV - 18 launches\n","  LVM3 - 9 launches\n","\n","Consolidated dataset:\n","  Total missions - 91\n","  Date range - 20 September 1993 05:12 to 30 July 2025 12:10 [ 68 ]\n","\n","Outcome distribution:\n","launch_outcome\n","Success            80\n","Failure             8\n","Partial failure     3\n","Name: count, dtype: int64\n","\n","File saved - output/isro_launch_history_raw.csv\n"]}]},{"cell_type":"code","source":["# @title BLOCK 14 - Save All Files to Google Drive (RUN EVERY SESSION)\n","\n","# Note: Copy scraper package and datasets to Drive for persistence\n","\n","import shutil\n","import os\n","\n","print(\"=\" * 80)\n","print(\"SAVING TO GOOGLE DRIVE\")\n","print(\"=\" * 80)\n","\n","# Copy scraper package to Drive\n","drive_code_path = os.path.join(CODE_PATH, 'isro_scraper')\n","if os.path.exists(drive_code_path):\n","    shutil.rmtree(drive_code_path)\n","shutil.copytree('isro_scraper', drive_code_path)\n","print(f\"\\nScraper package saved to Drive\")\n","\n","# Copy consolidated dataset\n","shutil.copy(\n","    'output/isro_launch_history_raw.csv',\n","    os.path.join(DATASET_PATH, 'isro_launch_history_raw.csv')\n",")\n","print(f\"Consolidated dataset saved to Drive\")\n","\n","# Copy individual files\n","for filename in ['pslv_launches.csv', 'gslv_launches.csv', 'lvm3_launches.csv']:\n","    src = os.path.join('output', filename)\n","    dst = os.path.join(DATASET_PATH, 'individual', filename)\n","    shutil.copy(src, dst)\n","\n","print(f\"Individual datasets saved to Drive\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"ALL FILES SAVED TO GOOGLE DRIVE\")\n","print(\"=\" * 80)\n","print(f\"\\nLocations:\")\n","print(f\"  Code - {drive_code_path}\")\n","print(f\"  Dataset - {DATASET_PATH}\")\n","print(\"\\nData acquisition complete - ready for Notebook 02\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUwnuKk6Y6Cq","executionInfo":{"status":"ok","timestamp":1769465950918,"user_tz":-330,"elapsed":618,"user":{"displayName":"Apurva Upadhyay","userId":"15503606714773213523"}},"outputId":"0f52630c-2517-4d17-ac14-61d45cdfb9ad"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","SAVING TO GOOGLE DRIVE\n","================================================================================\n","\n","Scraper package saved to Drive\n","Consolidated dataset saved to Drive\n","Individual datasets saved to Drive\n","\n","================================================================================\n","ALL FILES SAVED TO GOOGLE DRIVE\n","================================================================================\n","\n","Locations:\n","  Code - /content/drive/My Drive/Course/Minor in AI/Final Project/ISRO Launch Trend Analysis - Apurva Upadhyay/Code/isro_scraper\n","  Dataset - /content/drive/My Drive/Course/Minor in AI/Final Project/ISRO Launch Trend Analysis - Apurva Upadhyay/Dataset\n","\n","Data acquisition complete - ready for Notebook 02\n"]}]}]}